services:
  # ---------- Kafka (single-broker, KRaft) ----------
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    environment:
      KAFKA_CFG_NODE_ID: "1"
      KAFKA_CFG_PROCESS_ROLES: "controller,broker"
      KAFKA_CFG_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_CFG_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      ALLOW_PLAINTEXT_LISTENER: "yes"
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list > /dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  # ---------- Flink session cluster ----------
  jobmanager:
    build:
      context: ./flink
    image: schiss-flink:1.19
    container_name: jobmanager
    command: jobmanager
    env_file:
      - ./.env
    environment:
      FLINK_PROPERTIES: "jobmanager.rpc.address: jobmanager"
    ports:
      - "8081:8081"
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped

  taskmanager:
    image: schiss-flink:1.19
    container_name: taskmanager
    command: taskmanager
    env_file:
      - ./.env
    environment:
      FLINK_PROPERTIES: "jobmanager.rpc.address: jobmanager"
      TASK_MANAGER_NUMBER_OF_TASK_SLOTS: "2"
    depends_on:
      - jobmanager
    restart: unless-stopped

  # Submit the PyFlink job once the JM is up
  flink-submit:
    image: schiss-flink:1.19
    container_name: flink-submit
    env_file:
      - ./.env
    depends_on:
      - jobmanager
    entrypoint: ["/bin/bash","-lc"]
    command: >
      set -e
      && until curl -fsS http://jobmanager:8081; do echo 'Waiting for Flink JM...'; sleep 3; done
      && /opt/flink/bin/flink run -d -py /opt/jobs/flink_job.py
    restart: "on-failure"

  # ---------- Producer: MySQL -> Kafka (RAW_TOPIC) ----------
  producer:
    build:
      context: ./producer
    container_name: mysql-producer
    env_file:
      - ./.env
    environment:
      # Tell the app to read the password from the mounted secret file
      MYSQL_PASSWORD_FILE: /run/secrets/mysql_password
      # (optional) override to be explicit
      KAFKA_BROKER: ${KAFKA_BROKER:-kafka:9092}
    depends_on:
      kafka:
        condition: service_healthy
    secrets:
      - mysql_password
    restart: unless-stopped

  # ---------- Consumer: Kafka (CURATED_TOPIC) -> Postgres ----------
  consumer:
    build:
      context: ./consumer
    container_name: postgres-consumer
    env_file:
      - ./.env
    environment:
      # Tell the app to read the password from the mounted secret file
      PG_PASSWORD_FILE: /run/secrets/pg_password
      # (optional) override to be explicit
      KAFKA_BROKER: ${KAFKA_BROKER:-kafka:9092}
    depends_on:
      kafka:
        condition: service_healthy
    secrets:
      - pg_password
    restart: unless-stopped

# ─── Secrets (files are read at container path /run/secrets/<name>) ─────────────
secrets:
  mysql_password:
    file: ./secrets/mysql_password.txt
  pg_password:
    file: ./secrets/pg_password.txt
